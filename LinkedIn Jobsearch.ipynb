{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1ca0877-27a5-4960-89ce-1dde98ce0b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from IPython.display import HTML, IFrame\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import logging\n",
    "from linkedin_api import Linkedin\n",
    "import configparser\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5086ccac-555d-447e-a48e-fde522664467",
   "metadata": {},
   "outputs": [],
   "source": [
    "level = logging.DEBUG\n",
    "logging.basicConfig(filename='example.log', filemode='w', encoding='utf-8', level=level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa363e2a-f0a3-4cf3-89fd-c5ba2ba392a8",
   "metadata": {},
   "source": [
    "### Define necessary api requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64b90b-2511-49f6-aff3-2efd44558cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = configparser.ConfigParser()\n",
    "cfg.read('credentials.ini')\n",
    "api = Linkedin(cfg['credentials']['username'], cfg['credentials']['password'], authenticate=True) #only for company details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "629187d0-8904-4890-8db7-bcd25909160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89394a95-73d9-4afd-8f70-0d18eae04b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_last_request_time = 0\n",
    "def cached_exception_request(req_fn, cache_file_path, redownload=False, cache=True, retries=3, **kwargs):\n",
    "    global _last_request_time\n",
    "    logging.info(f'Checking cache for {cache_file_path}')\n",
    "    res_text = None\n",
    "    if redownload or not os.path.exists(cache_file_path):\n",
    "        logging.info(f'Requesting new {req_fn}')\n",
    "        success = False\n",
    "        tries = 0\n",
    "        while not success and tries < retries:\n",
    "            time_between_requests = 0.9+random.random()*2.1\n",
    "            logging.info(\"Sending request...\")\n",
    "            try:\n",
    "                current_time = time.time()\n",
    "                if current_time - _last_request_time < time_between_requests:\n",
    "                    logging.info(f\"Sleeping {time_between_requests - (current_time - _last_request_time)} seconds before request\")\n",
    "                    time.sleep(time_between_requests - (current_time - _last_request_time))\n",
    "                _last_request_time = time.time()\n",
    "                res = req_fn(**kwargs)\n",
    "                logging.info(f\"Received code: {res.status_code}\")\n",
    "                if res.status_code == 200 or res.status_code == 302:\n",
    "                    logging.info(\"success\")\n",
    "                    success = True\n",
    "                    res_text = res.text\n",
    "                    if cache:\n",
    "                        logging.info(f\"saving to file {cache_file_path}\")                        \n",
    "                        with open(cache_file_path, 'w') as cf:\n",
    "                            cf.write(res_text)\n",
    "                elif str(res.status_code).startswith('4'):\n",
    "                    logging.error(\"Request failed. Trying again.\")\n",
    "                elif str(res.status_code).startswith('5'):\n",
    "                    logging.error(\"Request failed. Trying again.\")\n",
    "                elif res.status_code == 999:\n",
    "                    logging.error('Request is faulty? (missing cookies etc?)')\n",
    "                    print('Try logging in for this')\n",
    "                    break\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logging.error(\"Python requests error:\", e)            \n",
    "            tries += 1\n",
    "            \n",
    "    else:\n",
    "        logging.info(f'Loading cached entry {req_fn}')\n",
    "        with open(cache_file_path, 'r') as cf:\n",
    "            res_text = cf.read()\n",
    "    return res_text\n",
    "\n",
    "def cached_exception_api_request(req_fn, cache_file_path, redownload=False, cache=True, retries=3, **kwargs):\n",
    "    global _last_request_time\n",
    "    logging.info(f'Checking cache for {cache_file_path}')\n",
    "    res_text = None\n",
    "    if redownload or not os.path.exists(cache_file_path):\n",
    "        logging.info(f'Requesting new {req_fn}')\n",
    "        success = False\n",
    "        tries = 0\n",
    "        while not success and tries < retries:\n",
    "            time_between_requests = 0.9+random.random()*2.1\n",
    "            logging.info(\"Sending request...\")\n",
    "            try:\n",
    "                current_time = time.time()\n",
    "                if current_time - _last_request_time < time_between_requests:\n",
    "                    logging.info(f\"Sleeping {time_between_requests - (current_time - _last_request_time)} seconds before request\")\n",
    "                    time.sleep(time_between_requests - (current_time - _last_request_time))\n",
    "                _last_request_time = time.time()\n",
    "                res = req_fn(**kwargs)\n",
    "                success = True\n",
    "                res_text = res\n",
    "                if cache:\n",
    "                    logging.info(f\"saving to file {cache_file_path}\")                        \n",
    "                    with open(cache_file_path, 'w') as cf:\n",
    "                        json.dump(res_text, cf)\n",
    "            except (requests.exceptions.RequestException, json.JSONDecodeError, KeyError) as e:\n",
    "                logging.error(\"Python requests error:\", e)            \n",
    "            tries += 1\n",
    "            \n",
    "    else:\n",
    "        logging.info(f'Loading cached entry {req_fn}')\n",
    "        with open(cache_file_path, 'r') as cf:\n",
    "            res_text = json.load(cf)\n",
    "    return res_text\n",
    "\n",
    "def job_search(keyword_list:list, location:list, geoId:str='', locationId:str='', distance=25, filter_company:list=[], filter_geoId:list=[], filter_jobtype=['F'], filter_joblevel=['2'], pageNum=0, **kwargs):\n",
    "\n",
    "    headers = {\n",
    "        'DNT': '1',\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.64 Safari/537.36',\n",
    "        'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"101\", \"Google Chrome\";v=\"101\"',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'sec-ch-ua-platform': '\"Linux\"',\n",
    "    }\n",
    "    base_url = 'https://www.linkedin.com/jobs/search?'\n",
    "    params = {}\n",
    "    if keyword_list:\n",
    "        params['keywords']=\"+\".join(keyword_list)\n",
    "    if location:\n",
    "        params['location']=\"+\".join(location)\n",
    "    params['locationId']=locationId\n",
    "    params['f_TPR']=''\n",
    "    if geoId:\n",
    "        params['geoId']=geoId\n",
    "    if distance:\n",
    "        params['distance']=distance\n",
    "    if filter_company:\n",
    "        params['f_C']=\",\".join(filter_company)\n",
    "    if filter_geoId:\n",
    "        params['f_PP']=\",\".join(filter_geoId)\n",
    "    if filter_jobtype:\n",
    "        params['f_JT']=\",\".join(filter_jobtype)\n",
    "    if filter_joblevel:\n",
    "        params['f_E']=\",\".join(filter_joblevel)\n",
    "    if pageNum:\n",
    "        params['pageNum']=pageNum\n",
    "    joined_params = params | kwargs\n",
    "    param_string = '&'.join([f'{k}={v}' for k,v in joined_params.items()])\n",
    "    return session.get(base_url + param_string, allow_redirects=False, headers=headers)\n",
    "\n",
    "def job_search_scroll(keyword_list:list, location:list, geoId:str='', locationId:str='', distance=25, filter_company:list=[], filter_geoId:list=[], filter_jobtype=['F'], filter_joblevel=['2'], pageNum=0, **kwargs):\n",
    "    \n",
    "    headers = {\n",
    "        'DNT': '1',\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.64 Safari/537.36',\n",
    "        'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"101\", \"Google Chrome\";v=\"101\"',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'sec-ch-ua-platform': '\"Linux\"',\n",
    "    }\n",
    "    base_url = 'https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?'\n",
    "    params = {}\n",
    "    if keyword_list:\n",
    "        params['keywords']=\"+\".join(keyword_list)\n",
    "    if location:\n",
    "        params['location']=\"+\".join(location)\n",
    "    params['locationId']=locationId\n",
    "    params['f_TPR']=''\n",
    "    if geoId:\n",
    "        params['geoId']=geoId\n",
    "    if distance:\n",
    "        params['distance']=distance\n",
    "    if filter_company:\n",
    "        params['f_C']=\",\".join(filter_company)\n",
    "    if filter_geoId:\n",
    "        params['f_PP']=\",\".join(filter_geoId)\n",
    "    if filter_jobtype:\n",
    "        params['f_JT']=\",\".join(filter_jobtype)\n",
    "    if filter_joblevel:\n",
    "        params['f_E']=\",\".join(filter_joblevel)\n",
    "    if pageNum:\n",
    "        params['pageNum']=pageNum\n",
    "    joined_params = params | kwargs\n",
    "    param_string = '&'.join([f'{k}={v}' for k,v in joined_params.items()])\n",
    "    return session.get(base_url + param_string, allow_redirects=False, headers=headers)\n",
    "\n",
    "def job_full_entry(url):\n",
    "    return requests.get(url)\n",
    "\n",
    "def job_inner_entry(jobId, refId, trackingId):\n",
    "    return session.get(f'https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{jobId}?refId={refId}&trackingId={trackingId}')\n",
    "\n",
    "def company_request(name):\n",
    "    return api.get_company(name)\n",
    "# def company_request(url):\n",
    "#     cookies = {\n",
    "#     'lang': 'v=2&lang=en-us' #Fix to english\n",
    "#     }\n",
    "\n",
    "#     headers = {\n",
    "#         'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "#         'Accept-Language': 'de-DE,de;q=0.9',\n",
    "#         'Cache-Control': 'max-age=0',\n",
    "#         'Connection': 'keep-alive',\n",
    "#         'DNT': '1',\n",
    "#         'Referer': 'https://www.linkedin.com/',\n",
    "#         'Sec-Fetch-Dest': 'document',\n",
    "#         'Sec-Fetch-Mode': 'navigate',\n",
    "#         'Sec-Fetch-Site': 'same-site',\n",
    "#         'Sec-Fetch-User': '?1',\n",
    "#         'Upgrade-Insecure-Requests': '1',\n",
    "#         'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.64 Safari/537.36',\n",
    "#         'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"101\", \"Google Chrome\";v=\"101\"',\n",
    "#         'sec-ch-ua-mobile': '?0',\n",
    "#         'sec-ch-ua-platform': '\"Linux\"',\n",
    "#     }\n",
    "\n",
    "#     return session.get(url, headers=headers, cookies=cookies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40b00ca8-1e98-4fd0-84d6-cc6968552503",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# side_panel = bs.find('section', class_=re.compile('two-pane-serp-page__detail-view')).find('div', re.compile(\"details-pane__content.*\"))\n",
    "# side_panel.insert(0, job_inner_entry(jobId, refId, trackingId).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03fda4e-b02d-4985-8670-6a3e20a4c181",
   "metadata": {},
   "source": [
    "## Prelim\n",
    "- Define job query (with offset as input)\n",
    "- Get number of jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "858df918-c0a9-4cc2-bc8a-841ac71f3c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    'keyword_list' : 'Data Scientist'.split(' '),\n",
    "    'location' : 'Düsseldorf, Nordrhein-Westfalen, Deutschland'.split(' '),\n",
    "    'geoId' : '106491352',\n",
    "    'position' : 1,\n",
    "    'pageNum' : 0\n",
    "}\n",
    "def retrieve_jobs_fn(start):\n",
    "    return job_search_scroll(**query, start=start)\n",
    "#retrieve_jobs_fn = lambda p: job_search_scroll(**query, start=p)\n",
    "\n",
    "ret = cached_exception_request(job_search, 'output/main_page.html', redownload=True, **query)\n",
    "bsg = BeautifulSoup(ret)\n",
    "job_count = int(bsg.find('span', \"results-context-header__job-count\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a7e522c4-30d4-410e-b7ff-6891d347462c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#currentJobId=3130239556&distance=25&f_E=2&f_F=it%2Ceng%2Cbd%2Csale%2Crsch%2Canls%2Cdist%2Chr&f_JT=F&f_TPR=r604800&f_WT=1%2C3&geoId=106491352&keywords=machine%20learning%20engineer%20python&sortBy=R\n",
    "query = {\n",
    "    'keyword_list' : 'Machine Learning Engineer Python'.split(' '),\n",
    "    'currentJobId': 3130239556,\n",
    "    'location' : None, #'Düsseldorf, Nordrhein-Westfalen, Deutschland'.split(' '),\n",
    "    'geoId' : '106491352',\n",
    "    'distance': '25',\n",
    "    'f_E': 2,\n",
    "    'f_WT': 1,\n",
    "    'f_JT': 'F',\n",
    "    'position' : 1,\n",
    "    'pageNum' : 0\n",
    "}\n",
    "def retrieve_jobs_fn(start):\n",
    "    return job_search_scroll(**query, start=start)\n",
    "#retrieve_jobs_fn = lambda p: job_search_scroll(**query, start=p)\n",
    "\n",
    "ret = cached_exception_request(job_search, 'output/main_page.html', redownload=True, **query)\n",
    "bsg = BeautifulSoup(ret)\n",
    "job_count = int(bsg.find('span', \"results-context-header__job-count\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb887e9e-99fc-4db6-9276-c1957a72b0ba",
   "metadata": {},
   "source": [
    "### Display Webpage (offline) with Ipython to visually verify query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5e42dc-b9fc-48f6-9e35-1ac665c7b1de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HTML(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df3d8a0-4d3e-4f1b-bf03-c94425d33b54",
   "metadata": {},
   "source": [
    "## Advanced filtering\n",
    "- filter companies based on anything (redefine company_filter_criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3de6debb-b37a-49f7-995a-1fb0a497ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Company Page Filters Cannot be used right now (Cant get company pages)\n",
    "# def company_filter_size_restriction(html, soup, infos):\n",
    "#     if 'Company size' in infos:\n",
    "#         c = infos['Company size'] == '10,001+ employees' or infos['Company size'] == '5001-10,000 employees' #Big companies\n",
    "#         if not c:\n",
    "#             print(infos['Name'], 'failed size restriction')\n",
    "#         return c\n",
    "#     return False\n",
    "\n",
    "# def company_filter_founded_restriction(html, soup, infos):\n",
    "#     if 'Company size' in infos:\n",
    "#         c = int(infos['Founded']) < 2020 # Only well established companies\n",
    "#         if not c:\n",
    "#             print(infos['Name'], 'failed founded restriction')\n",
    "#         return c\n",
    "#     return False\n",
    "\n",
    "# def company_filter_universal(html_text, filters, accum_fn=all):    \n",
    "#     bsc = BeautifulSoup(html_text)\n",
    "#     about_us = bsc.find('dl', class_='about-us__basic-info-list')\n",
    "#     name = bsc.find('h1', class_=re.compile('top-card-layout__title.*')).text\n",
    "#     infos = {div.dt.text.strip():div.dd.text.strip() for div in about_us.find_all('div', recursive=False)}\n",
    "#     infos['Name'] = name\n",
    "#     return accum_fn([f(html_text, bsc, infos) for f in filters])\n",
    "\n",
    "def company_json_filter_founded(comp_json):\n",
    "    if 'staffCount' in comp_json:\n",
    "        return comp_json['staffCount'] >= 200\n",
    "    return True #change to false if desired\n",
    "def company_json_filter_founded(comp_json):\n",
    "    if \"companyIndustries\" in comp_json:\n",
    "        return True #comp_json[\"companyIndustries\"][\"localizedName\"] in ['Chemicals']\n",
    "    return True #change to false if desired\n",
    "\n",
    "#Job Card Filters\n",
    "def jobcard_filter_title_restriction(card_soup):\n",
    "    job_title = card_soup.find('h3', class_='base-search-card__title').text\n",
    "    return not 'consult' in job_title.lower() #I don't want consulting\n",
    "\n",
    "\n",
    "def jobcard_filter_universal(card_soup, filters, accum_fn=all):\n",
    "    return accum_fn([f(card_soup) for f in filters])\n",
    "    \n",
    "# Job Description Filters\n",
    "def jobdescription_filter_consulting_restriction(html_text, soup, infos):\n",
    "    description = '\\n'.join(soup.find('div', class_='show-more-less-html__markup').find_all(text=True)).lower()\n",
    "    return not (\"consult\" in description or 'berater' in description or 'beratung' in description)\n",
    "\n",
    "def jobdescription_filter_master_restriction(html_text, soup, infos):\n",
    "    description = '\\n'.join(soup.find('div', class_='show-more-less-html__markup').find_all(text=True)).lower()\n",
    "    return \"master\" in description\n",
    "\n",
    "def jobdescription_filter_no_master_restriction(html_text, soup, infos):\n",
    "    description = '\\n'.join(soup.find('div', class_='show-more-less-html__markup').find_all(text=True)).lower()\n",
    "    return not \"master\" in description\n",
    "\n",
    "def jobdescription_filter_bachelor_restriction(html_text, soup, infos):\n",
    "    description = '\\n'.join(soup.find('div', class_='show-more-less-html__markup').find_all(text=True)).lower()\n",
    "    return \"bachelor\" in description\n",
    "\n",
    "def jobdescription_filter_no_bachelor_restriction(html_text, soup, infos):\n",
    "    description = '\\n'.join(soup.find('div', class_='show-more-less-html__markup').find_all(text=True)).lower()\n",
    "    return not \"bachelor\" in description\n",
    "\n",
    "def jobdescription_filter_universal(html_text, filters, accum_fn):\n",
    "    bsjd = BeautifulSoup(html_text)\n",
    "    return accum_fn([f(html_text, bsjd, None) for f in filters]) #TODO: Maybe parse universal info like for company filters\n",
    "\n",
    "#Use these functions if you want to download everyhting\n",
    "def always_yes(a):\n",
    "    return True\n",
    "def always_no(a):\n",
    "    return False\n",
    "    \n",
    "# company_filter_criteria = lambda t: company_filter_universal(t, [\n",
    "#     company_filter_size_restriction\n",
    "# ])\n",
    "# jobcard_filter_criteria = lambda s: jobcard_filter_universal(s, [\n",
    "#     jobcard_filter_title_restriction\n",
    "# ])\n",
    "# job_filter_criteria = lambda t: jobdescription_filter_universal(t, [\n",
    "#     jobdescription_filter_consulting_restriction\n",
    "# ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54344bfe-1a82-40cf-b479-605200ea8082",
   "metadata": {},
   "source": [
    "## Download jobs:\n",
    "1) Retrieve joblist (~25 at a time)\n",
    "3) Retrieve company Info\n",
    "2) Retrieve job descriptions (skips job descriptions already retrieved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ead5e5af-1dfb-40e2-b61c-896af6ef4ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_filter_criteria = always_yes\n",
    "company_json_filter_criteria = always_yes\n",
    "jobcard_filter_criteria = always_yes\n",
    "job_filter_criteria = always_yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a78e3a-648f-4583-ac60-2c19613ccee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_job = 0\n",
    "retrieved_job_entries = []\n",
    "while current_job < job_count:\n",
    "    fname = requests.utils.quote(f'retrieve_jobs?start={current_job}&{\"&\".join([f\"{k}={v}\" for k,v in query.items()])}', safe=\"%=&,+\")\n",
    "    retrieved_job_entries.append(f'output/{fname}.html')\n",
    "    page_res = cached_exception_request(\n",
    "        retrieve_jobs_fn,\n",
    "        retrieved_job_entries[-1],\n",
    "        redownload=True,\n",
    "        start=current_job\n",
    "    )\n",
    "    bsp = BeautifulSoup(page_res)\n",
    "    job_basecards = bsp.find_all('div', class_=re.compile(\"base-card relative.*\"))\n",
    "    current_job += len(job_basecards)\n",
    "    if len(job_basecards) == 0: #Safety measure (could make job_count obsolete)\n",
    "        print(\"no more jobs\")\n",
    "        break\n",
    "    for i in range(len(job_basecards)):\n",
    "        if jobcard_filter_criteria(job_basecards[i]): #Filter by job\n",
    "            jobId = job_basecards[i]['data-entity-urn'].split(':')[-1]\n",
    "            refId = job_basecards[i]['data-search-id']\n",
    "            trackingId = job_basecards[i]['data-tracking-id']\n",
    "            company_link = job_basecards[i].find('a', class_='hidden-nested-link')['href']\n",
    "            company_name = company_link.split('?')[0].split('/')[-1]\n",
    "            #comp_res = cached_exception_request(company_request, f'output/{company_name}.html', url=company_link)\n",
    "            comp_json = cached_exception_api_request(company_request, f'output/{company_name}.json', name=company_name)\n",
    "            if company_json_filter_criteria(comp_json):\n",
    "            #if company_filter_criteria(comp_res): #Filter based on criteria\n",
    "                job_res = cached_exception_request(job_inner_entry, f'output/{jobId}.html', jobId=jobId, refId=refId, trackingId=trackingId)\n",
    "                if job_filter_criteria(job_res):\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d45a007-72f0-442c-9262-8b2ecea15018",
   "metadata": {},
   "source": [
    "## Display Jobs with filters\n",
    "1) get main page again (used cached from previous query)\n",
    "2) go through all (cached) jobs\n",
    "3) filter out any unwanted jobs and remove from HTML\n",
    "4) Display HTML ith IPython.Display\n",
    "5) Click to open in new tab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4da44bf1-b97f-4a7a-9c18-b1d220ea293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_page = cached_exception_request(job_search, 'output/main_page.html', redownload=False, **query)\n",
    "main_soup = BeautifulSoup(main_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6eec5706-3052-4c22-b7c4-bfe5444bf6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_filter_criteria = always_yes\n",
    "company_json_filter_criteria = always_yes\n",
    "jobcard_filter_criteria = always_yes\n",
    "job_filter_criteria = lambda t: jobdescription_filter_universal(t, [\n",
    "    #jobdescription_filter_consulting_restriction,\n",
    "    jobdescription_filter_master_restriction,\n",
    "    jobdescription_filter_no_bachelor_restriction,\n",
    "], accum_fn=all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b068e607-869d-40c6-9697-4a55b6be3189",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_list = main_soup.find('ul', {'class':re.compile('jobs[-_]+search[-_]+results[-_]+list')})\n",
    "job_list.clear()#delete jobs on start page (included in retrieved jobs)\n",
    "filtered_job_count = 0\n",
    "for retrieved_jobs in retrieved_job_entries:\n",
    "    with open(retrieved_jobs) as f:\n",
    "        job_results = BeautifulSoup(f.read())\n",
    "    for li in job_results.body.findChildren('li', recursive=False):\n",
    "        job_basecards = li.find_all('div', class_=re.compile(\"base-card relative.*\"))\n",
    "        if len(job_basecards) == 0: #Safety measure (could make job_count obsolete)\n",
    "            continue\n",
    "        for i in range(len(job_basecards)): #it's only one in this case\n",
    "            if jobcard_filter_criteria(job_basecards[i]): #Filter by job\n",
    "                jobId = job_basecards[i]['data-entity-urn'].split(':')[-1]\n",
    "                refId = job_basecards[i]['data-search-id']\n",
    "                trackingId = job_basecards[i]['data-tracking-id']\n",
    "                company_link = job_basecards[i].find('a', class_='hidden-nested-link')['href']\n",
    "                company_name = company_link.split('?')[0].split('/')[-1]\n",
    "                #comp_res = cached_exception_request(company_request, f'output/{company_name}.html', url=company_link)\n",
    "                if company_json_filter_criteria != always_yes:\n",
    "                    comp_json = cached_exception_api_request(company_request, f'output/{company_name}.json', name=company_name)\n",
    "                if company_json_filter_criteria(comp_json):\n",
    "                #if company_filter_criteria(comp_res): #Filter based on criteria\n",
    "                    job_res = cached_exception_request(job_inner_entry, f'output/{jobId}.html', jobId=jobId, refId=refId, trackingId=trackingId)\n",
    "                    if not job_filter_criteria(job_res):\n",
    "                        # li.decompose()\n",
    "                        pass\n",
    "                    else:\n",
    "                        job_list.append(li)\n",
    "                        filtered_job_count += 1\n",
    "main_soup.find('span', \"results-context-header__job-count\").string = str(filtered_job_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788c5c6e-556d-4574-b2f8-f0dd3519eea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(str(main_soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b5e7bf-2809-4002-9fb9-bb57b6c7361a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
